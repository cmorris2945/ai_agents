{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76cbecde",
   "metadata": {},
   "source": [
    "# Agent with Long-Time Memory\n",
    "* We will build an Agent that will help us to **manage a ToDo list**.\n",
    "* It will decide:\n",
    "    * **when to save items** to our ToDo list.\n",
    "    * **to save either a user profile or a collection of ToDo items**.\n",
    "* In addition to semantic memory (user facts), it will also have **procedural memory**.\n",
    "    * Remember, the procedural memory is the system prompt. This will allow the user to set preferences for creating ToDo items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c2f374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f124bc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3912161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ.get(\"Open_ai_key_here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54628e3",
   "metadata": {},
   "source": [
    "Lets install Langchain here...\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700d2831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961f51bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenA\n",
    "\n",
    "chatModel35 = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "chatModel4o = ChatOpenAI(model = \"gpt-4o\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c8cb60",
   "metadata": {},
   "source": [
    "Using TrustCall here...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70611337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Memory(BaseModel):\n",
    "    conent: str = Field(description= \"The main content of the memory. For example:\" \\\n",
    "    \"User expressed interest in learning about French language...\")\n",
    "\n",
    "class MemoryCollection(BaseModel):\n",
    "    memories: list[Memory] = Field(description=\"A list of memories about the user.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e57aad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trustcall import create_extractor\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "## Everytime you make instance of this class, you make a new list of tools used...\n",
    "\n",
    "class Spy:\n",
    "    def __init__(self):\n",
    "        self.called_tools = []\n",
    "\n",
    "    def __call__(self, run):\n",
    "        ### This is here will collect information about the tools used in the extractor...\n",
    "        q = [run]\n",
    "        while q:\n",
    "            r = q.pop()\n",
    "            if r.child_runs:\n",
    "                q.extend(r.child_runs)\n",
    "            if r.run_type == \"chat_model\":\n",
    "                self.called_tools.append(\n",
    "                    r.outputs[\"generations\"][0][0][\"message\"][\"kwargs\"][\"tool_calls\"]\n",
    "                    \n",
    "                )\n",
    "spy = Spy()\n",
    "\n",
    "##Initialize the model here....\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "## create the extractor here....\n",
    "\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Memory],\n",
    "    tool_choice= \"Memory\",\n",
    "    enable_inserts=True,\n",
    ")\n",
    "\n",
    "### Add the spy as a listener to the extractor....\n",
    "\n",
    "trustcall_extractor_see_all_tool_calls = trustcall_extractor.with_listeners(on_end=spy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78efecb",
   "metadata": {},
   "source": [
    "## Running Trustcall without \"listener\" to monitor the workflow tool call Just to test Trustcall..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a51fc40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbc56b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "\n",
    "\n",
    "## Instruction to the model to extract the memories from the user input...\n",
    "instruction = \"\"\"Extract memmories from the following converstion 'Mac'...\"\"\"\n",
    "\n",
    "### Converstion here....\n",
    "conversation =[HumanMessage(content=\"Hi I'm Chris\"),\n",
    "               AIMessage(content= \"Nice to meet you 'Jackass'\"),\n",
    "               HumanMessage(content=\"Yesterday I visited the local state prison to make fun of all the prisoners...\")]\n",
    "\n",
    "### Using the regular extractor here without the listener....\n",
    "result = trustcall_extractor.invoke(\"messages\": [SystemMessage(content= instruction)] + conversation})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Messages that contain tool calls....\n",
    "\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ffa29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Responses contain the memories that adhere to the schema....\n",
    "\n",
    "for m in result[\"responses\"]:\n",
    "    print(m)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52bd9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Metadata contains the tool calls used in the extraction process...\n",
    "\n",
    "for m in result[\"response_metadata\"]:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4467fbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update the conversation here...\n",
    "\n",
    "updated_conversation = [AIMessage(content=\"Thats great. what did you do after?\"),\n",
    "                        HumanMessage(content=\"I went to Tiburon and prepared a paella in the park\"),\n",
    "                        AIMessage(content=\"What else is on your mind?\"),\n",
    "                        HumanMessage(content=\"I was thinking about finally learning to cook paella for \" \\\n",
    "                        \"the sake of my girlfriend.\"),]\n",
    "\n",
    "## Update the  instruction here...\n",
    "\n",
    "system_instruction = \"\"\"Update existing memories and create new ones BASES on the followinf converstaion: \"\"\"\n",
    "\n",
    "\n",
    "## We'll save the existing memories, giving them an ID, key (tool name), and value....\n",
    "\n",
    "tool_name = \"Memory\"\n",
    "existing_memories= [(str(i), tool_name, memory.model_dump()) for i, memory in enumerate(result[\"responses\"])] if result[\"responses\"] else None\n",
    "existing_memories\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803d9382",
   "metadata": {},
   "source": [
    "## And now let's see Trustcall with the listener..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0c8305",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now here, we will use the extractor with the listener now.\n",
    "## We will envoke the extractor with the updated conversation and existing memories...\n",
    "\n",
    "result = trustcall_extractor_see_all_tool_calls.invoke({\"messages\": updated_conversation,\n",
    "                                                        \"existing\": existing_memories})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a19c7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Metadata contains the tool call....\n",
    "\n",
    "for m in result[\"response_metadata\"]:\n",
    "    print(m)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034c3f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Messages ccontain the tool calls here...\n",
    "\n",
    "for m in result[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83330b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parsed responses....\n",
    "\n",
    "for m in result[\"responses\"]:\n",
    "    print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4f36be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspet the tool calls made by the \"Trustcall\" extractor....\n",
    "\n",
    "spy.called_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b73670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "475783e8",
   "metadata": {},
   "source": [
    "## Ok, now in this section I will create the \"UpdateMemory\" class to select the element in the long-term memory we will update at one particular moment..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e927f79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## first import the \"TypedDict\" and the \"Literal\" classes from the \"typing\" module....\n",
    "from typing import TypedDict, Literal\n",
    "## make the UpdateMemory class and pass TypedDict as the base class...\n",
    "\n",
    "class UpdateMemory(TypedDict):\n",
    "    \"\"\"Decision on what memory type to update.\"\"\"\n",
    "    update_type:Literal[\"user\", \"todo\", \"instructions\"]\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb725ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b28633bf",
   "metadata": {},
   "source": [
    "## Now I will build the actual agent\n",
    "\n",
    "* I will use the router \"route_message\" to make a binary decision to save memories.\n",
    "* The memory collection updating will be handled by \"Trustcall\" in the write_memory node, like I did previously...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936b770b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from datetime import datetime\n",
    "from trustcall import create_extractor\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.messages import merge_message_runs, HumanMessage, SystemMessage\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraphm MessagesState, END, START\n",
    "from langgraph.store.base import BaseStore\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "##Initialize the chat model here....\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "\n",
    "## User Profile class definition here...\n",
    "\"\"\"So this here is the PROFILE of the user you are 'chatting' with...\"\"\"\n",
    "\n",
    "name: Optional[str] = Field(description= \"The user's name\", default= None)\n",
    "location: Optional[str] =Field(description=\"The user's location\", default= None)\n",
    "job: Optional[str] = Field(description=\"The user's job\", default= None)\n",
    "connections: list[str] = Field(\n",
    "    description= \"Personal connection of the user, such as familiy members, friends, or coworkers\",\n",
    "    default_factory=list\n",
    "\n",
    ")\n",
    "interests: list[str] = Field(\n",
    "    description =\"Interests that the user has\",\n",
    "    default_factory= list\n",
    ")\n",
    "\n",
    "\n",
    "## This is the \"To-Do\" Schema here...\n",
    "\n",
    "class ToDo(BaseModel):\n",
    "    task: str= Field(description=\"The task to be completed...\")\n",
    "    time_to_complete: Optional[int] = Field(description= \"Estimated time to complete the task (minutes)\")\n",
    "    deadline: Optional[datetime] =Field(\n",
    "        description=\"When the task needs to be completed by (if applicable)\",\n",
    "        default = None\n",
    "        \n",
    "    )\n",
    "    solutions: list[str] = Field(\n",
    "        description = \"List of specific, actionable solutions here like (specific ideas, service providers,\" \\\n",
    "        \"or concrete options relevant to completing the task)\",\n",
    "        min_items = 1,\n",
    "        default_factory=list\n",
    "    )\n",
    "\n",
    "    status: Literal[\"not started\", \"in progress\", \"done\", \"archived\"] = Field(\n",
    "        description=\"Current status of the task\",\n",
    "        default= \"not started\"\n",
    "    )\n",
    "\n",
    "\n",
    "### create the Trustcall extractor for updating the user profile here...\n",
    "\n",
    "profile_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Profile],\n",
    "    tool_choice=\"Profile\",\n",
    ")\n",
    "\n",
    "## Chabot instruction for choosing what to update and what tools to call...\n",
    "\n",
    "MODEL_SYSTEM_MESSAGE =\"\"\"You are a helpful chatbot.\n",
    "You are designed to be a companion to a user, helping them keep track of\n",
    "their \"TO-DO\" list.\n",
    "\n",
    "You have a 'long-term' memory ability which keeps track of three things...\n",
    "\n",
    "1. The user's profile (general information about them)\n",
    "2. The user's \"to-do\" list\n",
    "3. General instructions for updating the \"to-do\" list\n",
    "\n",
    "Here is the current User Profile (may be empty if no information has been collected yet.):\n",
    "\n",
    "<user_profil>\n",
    "{user_profile}\n",
    "</ser_profile>\n",
    "\n",
    "Here is the current \"to-do\" list. (may be empty if no tasks have been added yet):\n",
    "<todo>\n",
    "{todo}\n",
    "</todo>\n",
    "\n",
    "Here are the current user-specific preferences for updating to ToDo list. (May be empty if\n",
    "no preferences have been specified yet):\n",
    "<instructions>\n",
    "{instructions}\n",
    "</instructions>\n",
    "\n",
    "Here are your instructions for reasoning about the user's messages....\n",
    "\n",
    "1. Reason carefully about th user's messages as presented below.\n",
    "\n",
    "2. Decide whether any of your long-term memory should be updated:\n",
    "    - If personal information was provided about the user,\n",
    "    update the user's profile by calling UpdateMemory tool with type 'user'.\n",
    "    - If tasks are mentioned, update the ToDo list by calling UpdateMemory tool with type 'todo'.\n",
    "    - If the user has specific preferences for how to update the ToDo list, update the instructions\n",
    "    by calling UpdateMemory tool with type 'instructions'.\n",
    "\n",
    "3. Tell the user that you have updated your memory, if appropirate:\n",
    "    - Do not tell user you have updated the user's profile.\n",
    "    - Tell the user then, when you update the todo list.\n",
    "    - Do not tell the user that you have updated instructions.\n",
    "\n",
    "4. Err on the side of updating the todo list. No need to ask for explicit permission.\n",
    "\n",
    "5. Respond naturally to user after a tool call was made to save memories or if no\n",
    "tool call was made.\"\"\"\n",
    "\n",
    "### \n",
    "TRUSTCALL_INSTRUCTION = \"\"\"Reflect in the following interaction...\n",
    "User the provided tool list to retain any necessary memories about the user.\n",
    "User parallel tool calling to handle updates and insertions simultaneously.\n",
    "\n",
    "System Time: {time}\"\"\"\n",
    "\n",
    "### Instructions for updating the ToDo list:\n",
    "\n",
    "CREATE_INSTRUCTIONS= \"\"\"Reflect in the following.\n",
    "\n",
    "Based on this interaction, update your instructions for how to update ToDO list items.\n",
    "Use any feedback from the user to update how they like tp have items added, etc.\n",
    "\n",
    "Your current instructions are:\n",
    "\n",
    "<current_instructions>\n",
    "{current_instructions}\n",
    "</current_instructions>\"\"\"\n",
    "\n",
    "### Node definitions...\n",
    "\n",
    "def task_mAIstro(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "\n",
    "    \"\"\"Load memories from the store and use them to personalize the chatbot's response\"\"\"\n",
    "\n",
    "    ##Get the user ID from the config....\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "\n",
    "    ### Retrieve profile memory from the store...\n",
    "    namespace = (\"profile\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    if memories:\n",
    "        user_profile = memories[0].value\n",
    "    else:\n",
    "        user_profile= None\n",
    "\n",
    "    ## Retrieve task memory from the store....\n",
    "\n",
    "    namespace = (\"todo\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    todo = \"\\n\".join(f\"{mem.value}\" for mem in memories)\n",
    "\n",
    "    ## Retrieve custom instructions...\n",
    "\n",
    "    namespace = (\"instructions\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    if memories:\n",
    "        instructions = memories[0].value\n",
    "    else:\n",
    "        instructions = \"\"\n",
    "\n",
    "    system_msg = MODEL_SYSTEM_MESSAGE.format(user_profile= user_profile, todo=todo, instructions=instructions)\n",
    "\n",
    "    response = model.bind_tools([UpdateMemory], parallel_tool_calls=False).invoke([SystemMessage(content=system_msg)]+state[\"messages\"])\n",
    "\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def update_profile(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "                                                                                  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3b3a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b9cc82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf6b1f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccd18a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bd7493",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
